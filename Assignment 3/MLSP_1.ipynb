{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from random import sample\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = KeyedVectors.load_word2vec_format('word2vec-google-news-300.bin', binary=True)\n",
    "DEVICE=\"cuda:2\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset and Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, model, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last improvement.\n",
    "            min_delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.model = model\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset counter if loss improves\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.counter += 1  # Increase counter if no improvement\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepData():\n",
    "  f = pd.read_csv('/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/IMDB Dataset.csv')\n",
    "  #Encode the labels of the sentiment (positive and negative) to 0 and 1\n",
    "  le = LabelEncoder()\n",
    "  le.fit(f['sentiment'])\n",
    "  f['sentiment'] = le.transform(f['sentiment'])\n",
    "  #Split the data into training, validation and test\n",
    "  train_data, test_data, val_data = np.split(f.sample(frac=1), [int(.8*len(f)), int(.9*len(f))])\n",
    "  #split the data into 2 types, one for the review and one for the sentiment to create a data and label sort of structure\n",
    "  train_review = train_data['review']\n",
    "  train_sentiment = train_data['sentiment']\n",
    "  test_review = test_data['review']\n",
    "  test_sentiment = test_data['sentiment']\n",
    "  val_review = val_data['review']\n",
    "  val_sentiment = val_data['sentiment']\n",
    "  return train_review, train_sentiment, test_review, test_sentiment, val_review, val_sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom dataset to take the sentances from the pandas dataframe, tokenize it and then put the word embeddings and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, review, sentiment, w2v_model, device):\n",
    "    self.review = review\n",
    "    self.sentiment = sentiment\n",
    "    self.w2v_model = w2v_model\n",
    "    self.device = device\n",
    "      \n",
    "  def __len__(self):\n",
    "    return len(self.review)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    review = self.review.iloc[index]\n",
    "    sentiment = self.sentiment.iloc[index]\n",
    "    #tokenize the review sentance\n",
    "    review = RegexpTokenizer(r'\\w+').tokenize(review)\n",
    "    #convert to word embeddings\n",
    "    review = [self.w2v_model[word] for word in review if word in self.w2v_model]\n",
    "    #convert list to numpy arrays to numpy arrays\n",
    "    review_numpy = np.array(review)\n",
    "    #convert the numpy arrays to tensors\n",
    "    review = torch.tensor(review_numpy, dtype = torch.float32, device = DEVICE)\n",
    "    return review, sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a new collate function to take care of different sequence lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    reviews, sentiments = zip(*batch)\n",
    "    #pack the reviews\n",
    "    device = reviews[0].device\n",
    "    reviews = torch.nn.utils.rnn.pack_sequence(reviews, enforce_sorted=False)\n",
    "    sentiments = torch.tensor(sentiments, dtype = torch.float32, device=device)\n",
    "    return reviews, sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataloader(device, w2v_model):\n",
    "  #Prepare the data\n",
    "  train_review, train_sentiment, test_review, test_sentiment, val_review, val_sentiment = prepData()\n",
    "  #Before creating the dataloader, let's fix the index of the train and validation data\n",
    "  train_review = train_review.reset_index(drop=True)\n",
    "  train_sentiment = train_sentiment.reset_index(drop=True)\n",
    "  val_review = val_review.reset_index(drop=True)\n",
    "  val_sentiment = val_sentiment.reset_index(drop=True)\n",
    "  #Create a train and validation dataloader for this dataset\n",
    "  train_dataloader = torch.utils.data.DataLoader(customDataset(train_review, train_sentiment, w2v_model,device), batch_size=32, shuffle=True, collate_fn=custom_collate)\n",
    "  validation_dataloader = torch.utils.data.DataLoader(customDataset(val_review, val_sentiment, w2v_model, device), batch_size=32, shuffle=False, collate_fn=custom_collate)\n",
    "  test_dataloader = torch.utils.data.DataLoader(customDataset(test_review, test_sentiment, w2v_model, device), batch_size=32, shuffle=False, collate_fn=custom_collate)\n",
    "  return train_dataloader, validation_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training method to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(trainingDataloader, validationDataloader, model, optimizer, criterion, max_epochs) :\n",
    "  # Run the training loop for max_epochs\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  early_stopping = EarlyStopping(model,patience=3, min_delta=0.001)\n",
    "  for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    for batch_index, (batch_data, batch_labels) in enumerate(trainingDataloader):\n",
    "      optimizer.zero_grad()\n",
    "      output = model(batch_data)\n",
    "      loss = criterion(output, batch_labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      batch_train_loss.append(loss.item())\n",
    "      #Calculate the accuracy\n",
    "      predicted = torch.round(torch.sigmoid(output))\n",
    "      correct = (predicted == batch_labels).sum().item()\n",
    "      accuracy = correct / len(batch_labels)\n",
    "      batch_train_acc.append(accuracy)\n",
    "      print(f\"Epoch {epoch+1}/{max_epochs} batch {batch_index+1}/{len(trainingDataloader)} train loss {np.mean(batch_train_loss)}, train acc {accuracy}\", end='\\r')\n",
    "    print(f\"\\nEpoch {epoch+1}/{max_epochs} train loss {np.mean(batch_train_loss)}, train acc {np.mean(batch_train_acc)}\\n\", end='\\r')\n",
    "    train_loss.append(np.mean(batch_train_loss))\n",
    "    train_acc.append(np.mean(batch_train_acc))\n",
    "\n",
    "    #test the model on valiation data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      batch_val_loss = []\n",
    "      batch_val_acc = []\n",
    "      for batch_index, (batch_data, batch_labels) in enumerate(validationDataloader):\n",
    "        output = model(batch_data)\n",
    "        loss = criterion(output, batch_labels)\n",
    "        batch_val_loss.append(loss.item())\n",
    "        #Calculate the accuracy\n",
    "        predicted = torch.round(torch.sigmoid(output))\n",
    "        correct = (predicted == batch_labels).sum().item()\n",
    "        accuracy = correct / len(batch_labels)\n",
    "        batch_val_acc.append(accuracy)\n",
    "      print(f\"\\nEpoch {epoch+1}/{max_epochs} val loss {np.mean(batch_val_loss)}, val acc {np.mean(batch_val_acc)}\\n\", end='\\r')\n",
    "      val_loss.append(np.mean(batch_val_loss))\n",
    "      val_acc.append(np.mean(batch_val_acc))\n",
    "      # Check early stopping\n",
    "      early_stopping(np.mean(batch_val_acc), model)\n",
    "      #Commenting this since, we don't need to early stop, just save the best model\n",
    "      # if early_stopping.early_stop:\n",
    "      #     print(\"\\nEarly stopping triggered!\")\n",
    "      #     break  # Stop training\n",
    "  return train_loss, val_loss, train_acc, val_acc, early_stopping.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(testDataloader, model, name, file_path, criterion):\n",
    "  message = \"Starting Testing for \" + name\n",
    "  writeResults(file_path = file_path, message = message)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    for batch_index, (batch_data, batch_labels) in enumerate(testDataloader):\n",
    "      output = model(batch_data)\n",
    "      loss = criterion(output, batch_labels)\n",
    "      batch_test_loss.append(loss.item())\n",
    "      #Calculate the accuracy\n",
    "      predicted = torch.round(torch.sigmoid(output))\n",
    "      correct = (predicted == batch_labels).sum().item()\n",
    "      accuracy = correct / len(batch_labels)\n",
    "      batch_test_acc.append(accuracy)\n",
    "    message_test = f\"\\nTest loss {np.mean(batch_test_loss)}, Test acc {np.mean(batch_test_acc)}\\n\"\n",
    "    writeResults(message=message_test, file_path=file_path)\n",
    "    print(message_test, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFig(train_loss, val_loss, name):\n",
    "  #Plot the training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    path = '/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/'\n",
    "    location = path + name + '.png'\n",
    "    plt.savefig(location)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResults(file_path, message):\n",
    "  # Save to a file\n",
    "  with open(file_path, \"a\") as f:\n",
    "      f.write(message + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 PCA Semantic Clustering\n",
    "Use pretrained word2vec to create 300 word embeddings\n",
    "use PCA to reduce dimentions from 300 to 2\n",
    "Make a scatter plot for 10 validation sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the sentances, remove the stop words and the redundant words and embedded the unique words to form a matrix\n",
    "#Sentances is a pandas dataframe of sentances, model is the word to Vector model\n",
    "def sentence_to_matrix(sentences, model, emb_dim=300):\n",
    "  cachedStopWords = stopwords.words(\"english\")\n",
    "  #Generate a list of unique words by removing all the stop words\n",
    "  unique_words = []\n",
    "  indices = sentences.index.values\n",
    "  for i in indices:\n",
    "    sentance  = RegexpTokenizer(r'\\w+').tokenize(sentences[i])\n",
    "    [unique_words.append(word.lower()) for word in sentance if (word not in cachedStopWords and word.lower() not in unique_words)]\n",
    "  #Embedd the tokens\n",
    "  embedded_tokens = np.zeros((len(unique_words), emb_dim))\n",
    "  for i, word in enumerate(unique_words):\n",
    "    if word in model:\n",
    "      embedded_tokens[i] = model[word]\n",
    "  return embedded_tokens, unique_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a PCA function to reduce the embedding dimentions to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n_components=2):\n",
    "  # Calculate the covariance matrix\n",
    "  cov_matrix = np.cov(data.T)\n",
    "  # Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "  eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "  #Generate the transformation matrix\n",
    "  transformation_matrix = eigenvectors[:, :n_components]\n",
    "  #Transform the data\n",
    "  transformed_data = np.dot(data, transformation_matrix)\n",
    "  return transformed_data, transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createData():\n",
    "  print(\"Read Data\")\n",
    "  f = pd.read_csv('/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/IMDB Dataset.csv')\n",
    "  #Encode the labels of the sentiment (positive and negative) to 0 and 1\n",
    "  print(\"Encoding the labels\")\n",
    "  le = LabelEncoder()\n",
    "  le.fit(f['sentiment'])\n",
    "  f['sentiment'] = le.transform(f['sentiment'])\n",
    "  \n",
    "  #Split the data into training, validation and test\n",
    "  print(\"train val test split\")\n",
    "  train_data, test_data, val_data = np.split(f.sample(frac=1), [int(.8*len(f)), int(.9*len(f))])\n",
    "  \n",
    "  #split the data into 2 types, one for the review and one for the sentiment to create a data and label sort of structure\n",
    "  print(\"X and Y split\")\n",
    "  train_review = train_data['review']\n",
    "  train_sentiment = train_data['sentiment']\n",
    "  test_review = test_data['review']\n",
    "  test_sentiment = test_data['sentiment']\n",
    "  val_review = val_data['review']\n",
    "  val_sentiment = val_data['sentiment']\n",
    "  #Generate the 100 dimention word-embedings for the training, validation and the test datasets\n",
    "  print(\"sentance to matrix on training\")\n",
    "  train_embed, unique_words_train = sentence_to_matrix(train_review, w2v_model)\n",
    "  #Generate the 100 dimention word-embedings for the validation\n",
    "  print(\"sentance to matrix on validation\")\n",
    "  valid_embed, unique_words_valid = sentence_to_matrix(val_review, w2v_model)\n",
    "  #Generate the 100 dimention word-embedings for the test\n",
    "  print(\"sentance to matrix on test\")\n",
    "  test_embed, unique_words_test= sentence_to_matrix(test_review, w2v_model)\n",
    "  return train_embed, valid_embed, unique_words_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pca():\n",
    "  print(\"Creating Data\")\n",
    "  train_embed, valid_embed, unique_words_valid = createData()\n",
    "  print(\"PCA begin on trainging data\")\n",
    "  train_embed_2d, transformation_matrix = pca(train_embed)\n",
    "  # perform PCA on the val dataset and plot the 2d plot for the words\n",
    "  print(\"PCA on validation\")\n",
    "  val_embed_2d = np.dot(valid_embed, transformation_matrix)\n",
    "  #Take maximum 20 words to show\n",
    "  unique_words_short = unique_words_valid\n",
    "  val_embed_2d_short = val_embed_2d\n",
    "  # Plot the val_embed_2d embedding points with the text\n",
    "  print(\"Plotting\")\n",
    "  plt.figure(figsize=(40,25))\n",
    "  plt.scatter(val_embed_2d_short[:, 0], val_embed_2d_short[:, 1])\n",
    "  for i, word in enumerate(unique_words_short):\n",
    "      plt.annotate(word, (val_embed_2d_short[i, 0], val_embed_2d_short[i, 1]))\n",
    "  plt.show()\n",
    "  path = '/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/'\n",
    "  location = path + \"cluster\" + '.png'\n",
    "  plt.savefig(location)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.2 LSTM Model<br>\n",
    "Create a LSTM model with 2 hidden layers, with 256 cells, followed by average<br>\n",
    "pooling and one-classification layer. It should take word2vec embedding as input.<br>\n",
    "### Prepare the Model and the train function<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should consist of an LSTM Model with 2 hidden dimentions and hidden size of 256,followed by average-pooling and one-classification layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "    super(LSTMModel, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "      #Forward function\n",
    "  def forward(self, x, hidden=None):\n",
    "    #if hidden is None then pass only the inputs to the lstm\n",
    "    if hidden is None:\n",
    "      output, output_hidden = self.lstm(x, hidden)\n",
    "    else:\n",
    "      output, output_hidden = self.lstm(x, hidden)\n",
    "    #pad the packed sequence that comes out of the lstm\n",
    "    output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0] #dim (batch_size, max_seq_len, hidden_size)\n",
    "    output = output.permute(0, 2, 1) #dim (batch_size, hidden_size, max_seq_len)\n",
    "    #pass the output through the average-pooling\n",
    "    pooled_output = self.avg_pool(output)#dim (batch_size, hidden_size, 1)\n",
    "    pooled_output = pooled_output.squeeze()#dim (batch_size, hidden_size)\n",
    "    # project the pooled output to the one dimention\n",
    "    projected_output = self.fc(pooled_output)\n",
    "    return projected_output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.3 Attention Based LSTM Model<br>\n",
    "Replace the average pooling layer in the above question with an attention based pooling after the 2-layer LSTM model.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same LSTM model as the above but the average pooling is replaced with attenion based pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAttention(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "    super(LSTMAttention, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_layers = num_layers\n",
    "    self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "    #Attention-based pooling\n",
    "    self.weight = nn.Linear(hidden_size,1)\n",
    "    self.fc = nn.Linear(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def forward(self, x, hidden = None):\n",
    "    if hidden is None:\n",
    "      output, output_hidden = self.lstm(x, hidden)\n",
    "    else:\n",
    "      output, output_hidden = self.lstm(x, hidden)\n",
    "    #pad the packed sequence that comes out of the lstm\n",
    "    output = nn.utils.rnn.pad_packed_sequence(output, batch_first=True)[0] #dim (batch_size, max_seq_len, hidden_size)\n",
    "    #pass the output through the attention-based pooling\n",
    "    attention_weights = torch.softmax(self.weight(output), dim=-1) #dim (batch_size, max_seq_len, 1)\n",
    "    pooled_output = torch.sum(output * attention_weights, dim=1) #dim (batch_size, hidden_size)\n",
    "    # project the pooled output to the one dimention\n",
    "    projected_output = self.fc(pooled_output).squeeze()\n",
    "    return projected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1.4 Transformer Based LSTM Model<br>\n",
    "Replace the two-layer LSTM model with a single transformer encoder layer with 256<br>\n",
    "hidden dimensions, followed by average pooling and classification with BCE loss.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lternate implementation of multi-head self attention head<br>\n",
    "Unlike the original implementation, which used the self attention head, I'd try to calculate the attention scores for the attention head without explicetly calling the self attention head.<br>\n",
    "I would try to ge the scores via matrix multiplicaiton itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention_2(torch.nn.Module):\n",
    "  def __init__(self, input_dim, num_heads):\n",
    "    super().__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.input_dim = input_dim\n",
    "    self.embedding_dim = input_dim//num_heads\n",
    "    self.weight_Q = nn.Linear(input_dim, self.num_heads * self.embedding_dim)\n",
    "    self.weight_K = nn.Linear(input_dim, self.num_heads * self.embedding_dim)\n",
    "    self.weight_V = nn.Linear(input_dim, self.num_heads * self.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #function to convert the Key, Query and Value matrices of dim (N, T, num_heads * embedding_dim) to dim dim (N, num_heads, T, embedding_dim)\n",
    "  def change_dimentions(self, matrix) :\n",
    "    new_shape = matrix.shape[:-1] + (self.num_heads, self.embedding_dim)\n",
    "    return matrix.reshape(new_shape).permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Define the forward function for the the input X which should be of the dimention (N, T, input_dim)\n",
    "  def forward(self, X):\n",
    "    # Define the Query, Key and Value vectors\n",
    "    Q = self.weight_Q(X)\n",
    "    K = self.weight_K(X)\n",
    "    V = self.weight_V(X)\n",
    "\n",
    "    # Change the dimentions of the Query, Key and Value vectors to account for the multi-head system\n",
    "    Q = self.change_dimentions(Q)\n",
    "    K = self.change_dimentions(K)\n",
    "    V = self.change_dimentions(V)\n",
    "\n",
    "    #Calculate the attention scores\n",
    "    scale = torch.sqrt(torch.tensor(self.embedding_dim, dtype=torch.float32, device=X.device))\n",
    "    attention_scores = Q @ K.transpose(-2, -1) / scale\n",
    "    self_attention_scores = nn.functional.softmax(attention_scores, dim = -1)# Should be of the dim (N, num_heads, T, T)\n",
    "\n",
    "    #Calculate the output embeddings\n",
    "    output_embeddings = self_attention_scores @ V # Should be of the dim (N, num_heads, T, embedding_dim)\n",
    "\n",
    "    #Reshape back to the original dimentions: dim (N, T, num_heads * embedding_dim) = dim (N, T, input_dim)\n",
    "    output_embeddings = output_embeddings.permute(0, 2, 1, 3)\n",
    "    output_embeddings = output_embeddings.reshape(output_embeddings.shape[:-2] + (self.num_heads * self.embedding_dim,))\n",
    "    return output_embeddings, self_attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's define a Multi-Layer Perceptron\n",
    "class MLP(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "    super().__init__()\n",
    "    self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "    self.activation = nn.GELU()\n",
    "    self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "#Define the forward pass with the the above defined weights\n",
    "# X is of the dim (N, T, input_dim)\n",
    "  def forward(self, X):\n",
    "    output = self.fc1(X)\n",
    "    output = self.activation(output)\n",
    "    output = self.fc2(output)\n",
    "    output = self.dropout(output)\n",
    "    return output # Output would also be of dim (T x input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we're ready to define a the Transformer encoder\n",
    "class TransformerEncoder(torch.nn.Module):\n",
    "  '''\n",
    "  # Define the init function with the\n",
    "  # input_dim being the dimention of the single input patch,\n",
    "  # embedding_dim being the dimention of the encoder context word\n",
    "  # num_heads being the number of the encoder blocks\n",
    "  # hidden_dim being the dim of the hidden layer of the MLP\n",
    "  # output_dim being the number of the classes\n",
    "  '''\n",
    "  def __init__(self, input_dim, num_heads, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.input_dim = input_dim\n",
    "    self.num_heads = num_heads\n",
    "    self.multihead_self_attention = MultiHeadSelfAttention_2(input_dim, num_heads)\n",
    "    self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "    self.mlp = MLP(input_dim, hidden_dim, input_dim)\n",
    "    self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "  # Define the forward function\n",
    "  # X would have the dim (N, T, input_dim)\n",
    "  def forward(self, X):\n",
    "    layer_normalization_1 = self.layer_norm1(X)#dim (batch_size, max_seq_len, input_dim)\n",
    "\n",
    "    #Calculate the Multi-head Attention Encoding for the patches\n",
    "    multihead_self_attention_output, self_attention_scores = self.multihead_self_attention(layer_normalization_1)\n",
    "    #dim (batch_size, max_seq_len, input_dim)\n",
    "\n",
    "    # Compute the residual Connection, will need to make sure that the dimentions of X and that of MSA match\n",
    "    residual_connection_1 = X + multihead_self_attention_output #dim (batch_size, max_seq_len, input_dim)\n",
    "    layer_normalization_2 = self.layer_norm2(residual_connection_1) #dim (batch_size, max_seq_len, input_dim)\n",
    "\n",
    "    #Calculate the MLP output from the Multi-layer perceptron\n",
    "    mlp_output = self.mlp(layer_normalization_2)#dim (batch_size, max_seq_len, input_dim)\n",
    "\n",
    "    # Compute the residual Connection, will need to make sure that the dimentions of X and that of MLP match\n",
    "    residual_connection_2 = residual_connection_1 + mlp_output#dim (batch_size, max_seq_len, input_dim)\n",
    "    return residual_connection_2, self_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTransformer(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_head, output_size, embedding_size = 256, max_seq_len = 512):\n",
    "    super(LSTMTransformer, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.max_seq_len = max_seq_len\n",
    "    self.projection = nn.Linear(input_size, embedding_size)\n",
    "    self.encoder = TransformerEncoder(embedding_size, num_head, hidden_size)\n",
    "    self.embedding = nn.Embedding(max_seq_len, embedding_size)\n",
    "    #Average Pooling\n",
    "    self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "    self.fc = nn.Linear(embedding_size, output_size)\n",
    "      #Dim of x = (batch_size, seq_len, input_dim)\n",
    "  def forward(self, x, hidden = None):\n",
    "    #pad the packed sequence that comes out of the lstm\n",
    "    x = nn.utils.rnn.pad_packed_sequence(x, batch_first=True)[0] #dim (batch_size, seq_len, input_size)\n",
    "\n",
    "    #limit the sequence length of the input to max_seq_len\n",
    "    seq_len = x.shape[1]\n",
    "    if(seq_len > self.max_seq_len):\n",
    "      x = x[:, :self.max_seq_len, :] #dim (batch_size, max_seq_len, input_size)\n",
    "\n",
    "    #project the data to embedding_dim \n",
    "    x = self.projection(x) #dim (batch_size, max_seq_len, embedding_size)\n",
    "\n",
    "    # Add the Projection embeddings\n",
    "    projection_vectors = torch.tensor([[j for j in range(x.shape[1])] for i in range (x.shape[0])], device = x.device) #dim (batch_size, max_seq_len)\n",
    "    projection_embeddings = self.embedding(projection_vectors)\n",
    "    x = x + projection_embeddings #dim (batch_size, max_seq_len, embedding_dim)\n",
    "   #pass the input throught the encoder\n",
    "    output = self.encoder(x)[0]#dim (batch_size, max_seq_len, embedding_dim)\n",
    "    \n",
    "    output = output.permute(0, 2, 1) #dim (batch_size, embedding_dim, max_seq_len)\n",
    "    #pass the output through the average-pooling\n",
    "    pooled_output = self.avg_pool(output)#dim (batch_size, embedding_dim, 1)\n",
    "    pooled_output = pooled_output.squeeze(-1)#dim (batch_size, embedding_dim)\n",
    "    # project the pooled output to the one dimention\n",
    "    projected_output = self.fc(pooled_output)\n",
    "    return projected_output.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train the model !!!\n",
    "<br>\n",
    "\n",
    "1.a: PCA Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\"\n",
    "  train_pca()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #__________________________________________________________________________________________\n",
    "  \n",
    "  input_size = 300\n",
    "  hidden_size = 256\n",
    "  num_layers = 2\n",
    "  output_size = 1\n",
    "  max_epochs = 1\n",
    "  learning_rate = 1e-4\n",
    "  weight_decay = 1e-5\n",
    "  file_path = \"/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/Results.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  print(\"Initializing the word2vec\")\n",
    "  nltk.download('punkt_tab')\n",
    "  nltk.download('stopwords')\n",
    "  #Prepare the data\n",
    "  print(\"Creating DataLoader\")\n",
    "  train_dataloader, validation_dataloader, test_dataloader = createDataloader(DEVICE, w2v_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #__________________________________________________________________________________________\n",
    "  '''1.b: LSTM with Average Pool'''\n",
    "  print(\"Working with LSTM Avg Pool\")\n",
    "  print(\"Creating model\")\n",
    "  model = LSTMModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "  model = model.to(device=DEVICE)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  #Prepare the data\n",
    "  print(\"Creating DataLoader\")\n",
    "  train_dataloader, validation_dataloader, test_dataloader = createDataloader(DEVICE, w2v_model)\n",
    "  model_name = \"LSTM_AVG_POOL\"\n",
    "  print(\"Start Training\")\n",
    "  train_loss, val_loss, train_acc, val_acc, model = trainModel(train_dataloader, validation_dataloader, model, optimizer, criterion, max_epochs)\n",
    "  print(\"End Training\")\n",
    "  \n",
    "  plotFig(train_loss=train_loss, val_loss=val_loss, name=model_name)\n",
    "  print(\"Start testing the model\")\n",
    "  #Test the model on testing data\n",
    "  testModel(test_dataloader, model,name = model_name, file_path = file_path, criterion=criterion)\n",
    "  print(\"End model testing\")\n",
    "  del model\n",
    "  del optimizer\n",
    "  del criterion\n",
    "  del train_dataloader\n",
    "  del validation_dataloader \n",
    "  del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________________________________________________________________________________\n",
    "  '''1.c: LSTM with Attention Model'''\n",
    "  print(\"Working with LSTM Attention model\")\n",
    "  print(\"Creating model\")\n",
    "  model = LSTMAttention(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, output_size=output_size)\n",
    "  model = model.to(device=DEVICE)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  #Prepare the data\n",
    "  print(\"Creating DataLoader\")\n",
    "  train_dataloader, validation_dataloader, test_dataloader = createDataloader(DEVICE, w2v_model)\n",
    "  model_name = \"LSTM_ATTENTION\"\n",
    "  print(\"Start Training\")\n",
    "  train_loss, val_loss, train_acc, val_acc, model = trainModel(train_dataloader, validation_dataloader, model, optimizer, criterion, max_epochs)\n",
    "  print(\"End Training\")\n",
    "  plotFig(train_loss=train_loss, val_loss=val_loss, name=model_name)\n",
    "  print(\"Start testing the model\")\n",
    "  #Test the model on testing data\n",
    "  testModel(test_dataloader, model, name = model_name, file_path = file_path, criterion=criterion)\n",
    "  print(\"End model testing\")\n",
    "  #Clean the GPU\n",
    "  del model\n",
    "  del optimizer\n",
    "  del criterion\n",
    "  del train_dataloader\n",
    "  del validation_dataloader \n",
    "  del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#__________________________________________________________________________________________\n",
    "  '''1.d: LSTM with Transformer Encoder'''\n",
    "  print(\"Working with Transformer based LSTM Model\")\n",
    "  print(\"Creating the model\")\n",
    "  model = LSTMTransformer(input_size=300, hidden_size=512, num_head=4, output_size=1)\n",
    "  model = model.to(device=DEVICE)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "  criterion = nn.BCEWithLogitsLoss()\n",
    "  print(\"Creating DataLoader\")\n",
    "  train_dataloader, validation_dataloader, test_dataloader = createDataloader(DEVICE, w2v_model)\n",
    "  model_name = \"LSTM_TRANSFORMER\"\n",
    "  print(\"Start Training\")\n",
    "  train_loss, val_loss, train_acc, val_acc, model = trainModel(train_dataloader, validation_dataloader, model, optimizer, criterion, max_epochs)\n",
    "  print(\"End Training\")\n",
    "  plotFig(train_loss=train_loss, val_loss=val_loss, name=model_name)\n",
    "  print(\"Start testing the model\")\n",
    "  #Test the model on testing data\n",
    "  testModel(testDataloader=test_dataloader, model=model, name = model_name, file_path = file_path, criterion=criterion)\n",
    "  print(\"End model testing\")\n",
    "  del model\n",
    "  del optimizer\n",
    "  del criterion\n",
    "  del train_dataloader\n",
    "  del validation_dataloader \n",
    "  del test_dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
