{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import librosa\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE=\"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "file_path = \"/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/Results_CNN.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, model, patience=5, min_delta=0.001):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last improvement.\n",
    "            min_delta (float): Minimum change to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float(\"inf\")\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.model = model\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset counter if loss improves\n",
    "            self.model = model\n",
    "        else:\n",
    "            self.counter += 1  # Increase counter if no improvement\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training method to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(trainingDataloader, validationDataloader, model, optimizer, criterion, max_epochs) :\n",
    "  # Run the training loop for max_epochs\n",
    "  train_loss = []\n",
    "  val_loss = []\n",
    "  train_acc = []\n",
    "  val_acc = []\n",
    "  early_stopping = EarlyStopping(model,patience=3, min_delta=0.001)\n",
    "  for epoch in range(max_epochs):\n",
    "    model.train()\n",
    "    batch_train_loss = []\n",
    "    batch_train_acc = []\n",
    "    for batch_index, (batch_data, batch_labels) in enumerate(trainingDataloader):\n",
    "      optimizer.zero_grad()\n",
    "      output = model(batch_data)\n",
    "      loss = criterion(output, batch_labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      batch_train_loss.append(loss.item())\n",
    "      #Calculate the accuracy\n",
    "      predicted = torch.argmax(output, dim=1)\n",
    "      correct = (predicted == batch_labels).sum().item()\n",
    "      accuracy = correct / len(batch_labels)\n",
    "      batch_train_acc.append(accuracy)\n",
    "      print(f\"Epoch {epoch+1}/{max_epochs} batch {batch_index+1}/{len(trainingDataloader)} train loss {np.mean(batch_train_loss)}, train acc {accuracy}\", end='\\r')\n",
    "    print(f\"\\nEpoch {epoch+1}/{max_epochs} train loss {np.mean(batch_train_loss)}, train acc {np.mean(batch_train_acc)}\\n\", end='\\r')\n",
    "    train_loss.append(np.mean(batch_train_loss))\n",
    "    train_acc.append(np.mean(batch_train_acc))\n",
    "\n",
    "    #test the model on valiation data if validation_dataloader is not None\n",
    "    if validationDataloader is not None:\n",
    "      model.eval()\n",
    "      with torch.no_grad():\n",
    "        batch_val_loss = []\n",
    "        batch_val_acc = []\n",
    "        for batch_index, (batch_data, batch_labels) in enumerate(validationDataloader):\n",
    "          output = model(batch_data)\n",
    "          loss = criterion(output, batch_labels)\n",
    "          batch_val_loss.append(loss.item())\n",
    "          #Calculate the accuracy\n",
    "          predicted = torch.argmax(output, dim=1)\n",
    "          correct = (predicted == batch_labels).sum().item()\n",
    "          accuracy = correct / len(batch_labels)\n",
    "          batch_val_acc.append(accuracy)\n",
    "        print(f\"\\nEpoch {epoch+1}/{max_epochs} val loss {np.mean(batch_val_loss)}, val acc {np.mean(batch_val_acc)}\\n\", end='\\r')\n",
    "        val_loss.append(np.mean(batch_val_loss))\n",
    "        val_acc.append(np.mean(batch_val_acc))\n",
    "        # Check early stopping\n",
    "        early_stopping(np.mean(batch_val_loss), model)\n",
    "\n",
    "        #Commenting this since, we don't need to early stop, just save the best model\n",
    "        # if early_stopping.early_stop:\n",
    "        #     print(\"\\nEarly stopping triggered!\")\n",
    "        #     break  # Stop training\n",
    "  if(validationDataloader is None):\n",
    "    return train_loss, val_loss, train_acc, val_acc, model\n",
    "  return train_loss, val_loss, train_acc, val_acc, early_stopping.model\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(testDataloader, model, name, criterion):\n",
    "  message = \"Starting Testing for \" + name\n",
    "  writeResults(message = message)\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    for batch_index, (batch_data, batch_labels) in enumerate(testDataloader):\n",
    "      output = model(batch_data)\n",
    "      loss = criterion(output, batch_labels)\n",
    "      batch_test_loss.append(loss.item())\n",
    "      #Calculate the accuracy\n",
    "      predicted = torch.argmax(output, dim=1)\n",
    "      correct = (predicted == batch_labels).sum().item()\n",
    "      accuracy = correct / len(batch_labels)\n",
    "      batch_test_acc.append(accuracy)\n",
    "    message_test = f\"\\nTest loss {np.mean(batch_test_loss)}, Test acc {np.mean(batch_test_acc)}\\n\"\n",
    "    writeResults(message=message_test)\n",
    "    print(message_test, end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFig(train_loss, val_loss, name):\n",
    "  #Plot the training and validation loss\n",
    "    plt.figure()\n",
    "    plt.plot(train_loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    path = '/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/'\n",
    "    location = path + name + '.png'\n",
    "    plt.savefig(location)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeResults(message):\n",
    "  # Save to a file\n",
    "  with open(file_path, \"a\") as f:\n",
    "      f.write(message + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.0 CNN Model<br>\n",
    "Make a CNN model architecture for this 10-class classification setting with the following details, <br>\n",
    "two layers of 2-D CNN with 16 filters 3 x3 size, with stride of 1 x1 and with max-pooling of 3x3. <br>\n",
    "Flatten the CNN outputs and use 2 fully connected layers of hidden dimensions 128 and then <br>\n",
    "classification with softmax non-linearity for 10 classes. Use the cross-entropy loss for training the models.<br>\n",
    "### Prepare the Model and the train function<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel_spectrogram(ap, n_mels=128, win_ms=25, hop_ms=10, duration=5, sr=44100):\n",
    "    y, sr = librosa.load(ap, sr=None, duration=duration)\n",
    "    win_length = int(win_ms * sr / 1000)\n",
    "    hop_length = int(hop_ms * sr / 1000)\n",
    "    \n",
    "    mel_spectrogram = librosa.feature.melspectrogram(\n",
    "        y=y,\n",
    "        sr=sr,\n",
    "        n_mels=n_mels,\n",
    "        win_length=win_length,\n",
    "        hop_length=hop_length,\n",
    "    )\n",
    "    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    return mel_spectrogram_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataset dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_df(df, audio_path):\n",
    "    features = []\n",
    "    labels = []\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        file_path = os.path.join(audio_path, row['filename'])\n",
    "        mel = extract_mel_spectrogram(file_path)\n",
    "        mel = mel[:, :500]  # Ensure shape is (128, 500)\n",
    "        features.append(mel)\n",
    "        labels.append(row['category'])\n",
    "    return np.expand_dims(np.array(features), axis=1), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a custom dataset for the mel spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDatasetCNN(torch.utils.data.Dataset):\n",
    "  def __init__(self, features, labels):\n",
    "    self.features = features\n",
    "    self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def __len__(self):\n",
    "    return len(self.features)\n",
    "  def __getitem__(self, index):\n",
    "    feature = self.features[index]\n",
    "    label = self.labels[index]\n",
    "    feature = torch.tensor(feature, dtype = torch.float32, device = DEVICE)\n",
    "    label = torch.tensor(label, dtype = torch.long, device = DEVICE)\n",
    "  \n",
    "    return feature, label\n",
    "def createDataloader_CNN(batch_size = 16):\n",
    "    meta_df = pd.read_csv('/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/ESC-50-master/meta/esc50.csv')\n",
    "    # Filter for ESC-10\n",
    "    esc10_df = meta_df[meta_df['esc10'] == True]\n",
    "    esc10_df = esc10_df.reset_index(drop=True)\n",
    "    #Encode the labels of the sentiment (positive and negative) to 0 and 1\n",
    "    le = LabelEncoder()\n",
    "    le.fit(esc10_df['category'])\n",
    "    esc10_df['category'] = le.transform(esc10_df['category'])\n",
    "    # Split data\n",
    "    train_df = esc10_df[esc10_df['fold'].isin([1, 2, 3])]\n",
    "    val_df   = esc10_df[esc10_df['fold'] == 4]\n",
    "    test_df  = esc10_df[esc10_df['fold'] == 5]\n",
    "    \n",
    "    #Define the audio file path\n",
    "    audio_path = '/data/home/saisuchithm/godwin/mlsp/Assignment/Assignment_3/ESC-50-master/audio'\n",
    "\n",
    "    # Extract all features\n",
    "    X_train, y_train = process_df(train_df, audio_path)\n",
    "    X_val, y_val     = process_df(val_df, audio_path)\n",
    "    X_test, y_test   = process_df(test_df, audio_path)\n",
    "\n",
    "    #Make the train, validation and test dataset\n",
    "    train_dataset = customDatasetCNN(X_train, y_train)\n",
    "    val_dataset = customDatasetCNN(X_val, y_val)\n",
    "    test_dataset = customDatasetCNN(X_test, y_test)\n",
    "\n",
    "    #Make the train, validation and test dataset\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN model for this 10 classification setting<br>\n",
    "ased on the type of norm{no_norm, batch_norm, layer_norm}, add that to the flatten output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCNN\u001b[39;00m(\u001b[43mnn\u001b[49m.Module):\n\u001b[32m      2\u001b[39m   \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_classes, norm = \u001b[33m'\u001b[39m\u001b[33mno_norm\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28msuper\u001b[39m(CNN, \u001b[38;5;28mself\u001b[39m).\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mNameError\u001b[39m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class CNN(nn.Module):\n",
    "  def __init__(self, num_classes, norm = 'no_norm'):\n",
    "    super(CNN, self).__init__()\n",
    "    #Conv-2d layer with  16 filters 3 ×3 size, with stride of 1 ×1\n",
    "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, stride=1)\n",
    "    #max-pooling of 3×3.\n",
    "    self.pool1 = nn.MaxPool2d(kernel_size=3)\n",
    "    #Conv-2 layer with  16 filters 3 ×3 size, with stride of 1 ×1\n",
    "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=1)\n",
    "    #max-pooling of 3×3.\n",
    "    self.pool2 = nn.MaxPool2d(kernel_size=3)\n",
    "    #  fully connected layers of hidden dimensions 128\n",
    "    self.fc1 = nn.Linear(in_features=11232, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128, out_features=num_classes)\n",
    "    # Choose the normalization\n",
    "    self.norm = nn.Identity()\n",
    "    if norm == 'batch_norm':\n",
    "      self.norm = nn.BatchNorm1d(11232)\n",
    "    elif norm == 'layer_norm':\n",
    "      self.norm = nn.LayerNorm(11232)\n",
    "    self.flatten = nn.Flatten()\n",
    "  def forward(self, x):\n",
    "    #Conv-2d layer with 16 filters 3 ×3 size, with stride of 1 ×1\n",
    "    x = f.relu(self.conv1(x))\n",
    "    #max-pooling of 3×3.\n",
    "    x = self.pool1(x)\n",
    "    #Conv-2d layer with 16 filters 3 ×3 size, with stride of 1 ×1\n",
    "    x = f.relu(self.conv2(x))\n",
    "    #max-pooling of 3×3.\n",
    "    x = self.pool2(x)\n",
    "    #Flatten the CNN outputs\n",
    "    x = self.flatten(x)\n",
    "    #If the norm is not None, appy the normalization\n",
    "    x = self.norm(x)\n",
    "\n",
    "    #pass throught the fully connected layer\n",
    "    x = f.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a Compare different optimizers\n",
    "  Compare the training and validation loss curves for training with (a) SGD, (b) SGD\n",
    "  with momentum (factor of 0.9), (c) RMSprop (with default parameters) and (d)\n",
    "  Adam optimizer.   '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check CNN training on different opimizers\n",
    "def optimizersCheck(train_dataloader, validation_dataloader, test_dataloader, model, criterion, max_epochs):\n",
    "  writeResults(message=\"Results for 3.a\")\n",
    "  optimizer1 = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "  optimizer2 = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "  optimizer3 = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "  optimizer4 = torch.optim.RMSprop(model.parameters(), lr = 1e-4)\n",
    "  optimizers = [optimizer1, optimizer2, optimizer3, optimizer4]\n",
    "  names = [\"SGD\", \"SGD with momentum\", \"Adam\", \"RMSProp\"]\n",
    "  for i in range(len(optimizers)):\n",
    "    model_copy = CNN(num_classes=10)\n",
    "    model_copy = model.to(device=DEVICE)\n",
    "    optimizer = optimizers[i]\n",
    "    name = names[i]\n",
    "    print(f\"Training with {name} optimizer\")\n",
    "    print(\"Start Training\")\n",
    "    train_loss, val_loss, _, _, model_copy = trainModel(trainingDataloader=train_dataloader\n",
    "                                                        , validationDataloader=validation_dataloader\n",
    "                                                        , model=model_copy\n",
    "                                                        , optimizer=optimizer\n",
    "                                                        , criterion=criterion\n",
    "                                                        , max_epochs=max_epochs)\n",
    "    print(\"End Training\")\n",
    "    plotFig(train_loss=train_loss, val_loss=val_loss, name=name)\n",
    "    print(\"Start testing the model\")\n",
    "    #Test the model on testing data\n",
    "    testModel(testDataloader=test_dataloader, model=model_copy, name = name, criterion=criterion)\n",
    "    print(\"End model testing\")\n",
    "    del model_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.b Comparing the normalization:<br>\n",
    "  At the flattened output and at the input of 2 fully connected layers, compare the<br>\n",
    "  training and validation loss curves for :<br>\n",
    "  a) No-norm<br>\n",
    "  b) Layer norm <br>\n",
    "  c) Batch norm. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Fucntion to run the model on the 3 normalization options: no_norm, batch_norm and layer_norm\n",
    "def compareNormalization(train_dataloader, validation_dataloader, test_dataloader, max_epochs):\n",
    "  writeResults(message=\"Results for 3.b\")\n",
    "  normalization = ['no_norm', 'batch_norm', 'layer_norm']\n",
    "  #Iterate through the normalization and calculate the model performance\n",
    "  for norm in normalization:\n",
    "    model = CNN(num_classes=10, norm = norm).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    print(f\"Training with {norm} normalization\")\n",
    "    print(\"Start Training\")\n",
    "    train_loss, val_loss, _, _, model_copy = trainModel(train_dataloader\n",
    "                                                        , validationDataloader=validation_dataloader\n",
    "                                                        , model=model\n",
    "                                                        , optimizer=optimizer\n",
    "                                                        , criterion=criterion\n",
    "                                                        , max_epochs=max_epochs)\n",
    "    print(\"End Training\")\n",
    "    plotFig(train_loss=train_loss, val_loss=val_loss, name=norm)\n",
    "    print(\"Start testing the model\")\n",
    "    #Test the model on testing data\n",
    "    testModel(testDataloader=test_dataloader, model=model_copy, name = norm, criterion=criterion)\n",
    "    print(\"End model testing\")\n",
    "    del model\n",
    "    del optimizer\n",
    "    del criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  ## 3.C Ensemble of methods<br>\n",
    "  For the first model, use SGD training without any normalization. <br>\n",
    "  For the second model use the RMSprop<br>\n",
    "  optimizer with Layer norm. <br>\n",
    "  For the third model, use Adam optimizer with Batch norm. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #function to train the ensemble methods\n",
    "def trainEnsembleMethods(train_dataloader, validation_dataloader, max_epochs):\n",
    "  #\n",
    "  # CNN Model with no normalization and SGD Training\n",
    "  model_1 = CNN(num_classes=10)\n",
    "  model_1 = model_1.to(device=DEVICE)\n",
    "  optimizer1 = torch.optim.SGD(model_1.parameters(), lr=1e-3)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  print(\"START Training the model_1 models\")\n",
    "  _, _, _, _, model_1 = trainModel(trainingDataloader = train_dataloader\n",
    "                                   , validationDataloader = validation_dataloader\n",
    "                                   , model = model_1\n",
    "                                   , optimizer = optimizer1\n",
    "                                   , criterion = criterion\n",
    "                                   , max_epochs=max_epochs)\n",
    "  print(\"END Training the model_1 models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  #CNN Model with layer norm and RMSProp training\n",
    "  model_2 = CNN(num_classes=10, norm = 'layer_norm')\n",
    "  model_2 = model_2.to(device=DEVICE)\n",
    "  optimizer2 = torch.optim.RMSprop(model_2.parameters(), lr=1e-4)\n",
    "  print(\"START Training the model_2 models\")\n",
    "  _, _, _, _, model_2 = trainModel(trainingDataloader = train_dataloader,\n",
    "                                    validationDataloader = validation_dataloader\n",
    "                                    , model = model_2\n",
    "                                    , optimizer = optimizer2\n",
    "                                    , criterion = criterion\n",
    "                                    , max_epochs=max_epochs)\n",
    "  print(\"END Training the model_2 models\")\n",
    "  #CNN Model with batch norm and Adam training\n",
    "  model_3 = CNN(num_classes=10, norm = 'batch_norm')\n",
    "  model_3 = model_3.to(device=DEVICE)\n",
    "  optimizer3 = torch.optim.Adam(model_3.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "  print(\"START Training the model_3 models\")\n",
    "  _, _, _, _, model_3 = trainModel(\n",
    "     trainingDataloader = train_dataloader,\n",
    "     validationDataloader = validation_dataloader,\n",
    "     model = model_3,\n",
    "     optimizer = optimizer3,\n",
    "     criterion = criterion,\n",
    "     max_epochs=max_epochs\n",
    "  )\n",
    "  print(\"END Training the model_3 models\")\n",
    "  del optimizer1\n",
    "  del optimizer2\n",
    "  del optimizer3\n",
    "  return model_1, model_2, model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Function to train and evaluate the ensemble average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensemble the model outputs using output averaging of the posterior model outputs from the three model outputs\n",
    "def ensembleAveraging(train_dataloader, validation_dataloader, test_dataloader, criterion, max_epochs):\n",
    "  print(\"START Training the Ensemble models\")\n",
    "  model_1, model_2, model_3 = trainEnsembleMethods(train_dataloader=train_dataloader\n",
    "                                                   , validation_dataloader= validation_dataloader\n",
    "                                                   , max_epochs= max_epochs)\n",
    "  print(\"END Training the Ensemble models\")\n",
    "  model_1.eval()\n",
    "  model_2.eval()\n",
    "  model_3.eval()\n",
    "  startMessage = f\"\\nStart Evaluation of Ensemble average\"\n",
    "  print(startMessage)\n",
    "  writeResults(message = startMessage)\n",
    "  with torch.no_grad():\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    for batch_index, (batch_data, batch_labels) in enumerate(test_dataloader):\n",
    "      output_1 = model_1(batch_data)\n",
    "      output_2 = model_2(batch_data)\n",
    "      output_3 = model_3(batch_data)\n",
    "      output = (output_1 + output_2 + output_3) / 3\n",
    "      loss = criterion(output, batch_labels)\n",
    "      batch_test_loss.append(loss.item())\n",
    "      #Calculate the accuracy\n",
    "      predicted = torch.argmax(output, dim=1)\n",
    "      correct = (predicted == batch_labels).sum().item()\n",
    "      accuracy = correct / len(batch_labels)\n",
    "      batch_test_acc.append(accuracy)\n",
    "    \n",
    "    endMessage = f\"Ensemble Average test loss {np.mean(batch_test_loss)}, Ensemble Average test acc {np.mean(batch_test_acc)}\"\n",
    "    print(endMessage)\n",
    "    writeResults(message = endMessage)\n",
    "    del model_1\n",
    "    del model_2\n",
    "    del model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble the model outputs using with optimal linear weighted combination of the three model outputs.<br>\n",
    "efine a new model that takes the model_1, model_2 and model_3 and gives out the weighted combination of the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleWeightedAverage(nn.Module):\n",
    "  def __init__(self, model_1, model_2, model_3):\n",
    "    super(EnsembleWeightedAverage, self).__init__()\n",
    "    self.model_1 = model_1\n",
    "    self.model_2 = model_2\n",
    "    self.model_3 = model_3\n",
    "    #Define 3 learnable parameters alpha, beta and gamma as the cofficients of the outputs of the model\n",
    "    self.weight = nn.Parameter(torch.tensor([1.0, 1.0, 1.0]))\n",
    "    #Freeze the weights for model_1, model_2, model_3\n",
    "    for model in [self.model_1, self.model_2, self.model_3]:\n",
    "       for param in model.parameters():\n",
    "          param.requires_grad = False\n",
    "    self.softmax = nn.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  def forward(self, x):\n",
    "    output1 = self.model_1(x)\n",
    "    output2 = self.model_2(x)\n",
    "    output3 = self.model_3(x)\n",
    "    normalized_weight = self.softmax(self.weight)\n",
    "    output = (normalized_weight[0] * output1 + normalized_weight[1] * output2 + normalized_weight[2] * output3)\n",
    "    return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a function to weighted Average ensemble method\n",
    "def trainEnsembleWeightedAverage(train_dataloader, validation_dataloader, max_epochs):\n",
    "  print(\"START Training the Ensemble models\\n\")\n",
    "  model_1, model_2, model_3 = trainEnsembleMethods(train_dataloader=train_dataloader\n",
    "                                                   , validation_dataloader= validation_dataloader\n",
    "                                                   , max_epochs= max_epochs)\n",
    "  print(\"END Training the Ensemble models\\n\")\n",
    "  model_1.eval()\n",
    "  model_2.eval()\n",
    "  model_3.eval()\n",
    "  print(\"START Training the Ensemble Weighted Average model\\n\")\n",
    "  model_weighted = EnsembleWeightedAverage(model_1, model_2, model_3)\n",
    "  optimizer = torch.optim.Adam(model_weighted.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  _, _, _, _, model_weighted = trainModel(trainingDataloader = validation_dataloader\n",
    "                                          , validationDataloader = None\n",
    "                                          , model = model_weighted\n",
    "                                          , optimizer = optimizer\n",
    "                                          , criterion = criterion\n",
    "                                          , max_epochs=max_epochs)\n",
    "  print(\"END Training the Ensemble Weighted Average model\\n\")\n",
    "  return model_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>\n",
    "Function to train and evaluate the ensemble weighted average<br>\n",
    "<br>\n",
    "Ensemble the model outputs using with optimal linear weighted combination of the three model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensembleWeightedAverage(train_dataloader, validation_dataloader, test_dataloader, max_epochs):\n",
    "  #Define the ensemble weighted average model \n",
    "  model_weighted = trainEnsembleWeightedAverage(train_dataloader=train_dataloader\n",
    "                                                , validation_dataloader=validation_dataloader\n",
    "                                                , max_epochs=max_epochs)\n",
    "  model_weighted.eval()\n",
    "  startMessage = f\"\\nStart Evaluation of Ensemble weighted average\"\n",
    "  print(startMessage)\n",
    "  writeResults(message = startMessage)\n",
    "  with torch.no_grad():\n",
    "    batch_test_loss = []\n",
    "    batch_test_acc = []\n",
    "    for batch_index, (batch_data, batch_labels) in enumerate(test_dataloader):\n",
    "      output = model_weighted(batch_data)\n",
    "      loss = criterion(output, batch_labels)\n",
    "      batch_test_loss.append(loss.item())\n",
    "      #Calculate the accuracy\n",
    "      predicted = torch.argmax(output, dim=1)\n",
    "      correct = (predicted == batch_labels).sum().item()\n",
    "      accuracy = correct / len(batch_labels)\n",
    "      batch_test_acc.append(accuracy)\n",
    "    weights = torch.nn.functional.softmax(model_weighted.weight)  \n",
    "    endMessage = f\"Ensemble test loss {np.mean(batch_test_loss)}, Ensemble test acc {np.mean(batch_test_acc)}, weights are {weights}\"\n",
    "    print(endMessage)\n",
    "    writeResults(message = endMessage)\n",
    "    del model_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Train the model !!!\n",
    "<br>\n",
    "if __name__ == \"__main__\":<br>\n",
    "    learning_rate = 1e-4<br>\n",
    "    weight_decay = 1e-5<br>\n",
    "    num_classes = 10<br>\n",
    "    model = CNN(num_classes=num_classes)<br>\n",
    "    model = model.to(device=DEVICE)<br>\n",
    "    max_epochs = 10<br>\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)<br>\n",
    "    # optimizer = torch.optim.RMSprop(model.parameters(), lr = 1e-4)<br>\n",
    "    <br>\n",
    "    criterion = nn.CrossEntropyLoss()<br>\n",
    "    print(\"Creating DataLoader\")<br>\n",
    "    train_dataloader, validation_dataloader, test_dataloader = createDataloader_CNN()<br>\n",
    "  <br>\n",
    "    #3.a Compare the Optimizers<br>\n",
    "    optimizersCheck(train_dataloader, validation_dataloader, test_dataloader,model, criterion, max_epochs)<br>\n",
    "    #3.b Compare the Normalization<br>\n",
    "    compareNormalization(train_dataloader, validation_dataloader, test_dataloader, max_epochs)<br>\n",
    "    #3.c Compare the Ensemble methods<br>\n",
    "    ##Ensemble Average<br>\n",
    "    ensembleAveraging(train_dataloader=train_dataloader<br>\n",
    "                      , validation_dataloader=validation_dataloader<br>\n",
    "                      , test_dataloader= test_dataloader<br>\n",
    "                      , criterion= criterion<br>\n",
    "                      , max_epochs= max_epochs)<br>\n",
    "    ##Ensemble Weighted Average<br>\n",
    "    ensembleWeightedAverage(train_dataloader=train_dataloader<br>\n",
    "                            , validation_dataloader=validation_dataloader<br>\n",
    "                            , test_dataloader=test_dataloader<br>\n",
    "                            , max_epochs=max_epochs)<br>\n",
    "    #Clear GPU<br>\n",
    "    del model<br>\n",
    "    del optimizer<br>\n",
    "    del criterion<br>\n",
    "    del train_dataloader<br>\n",
    "    del validation_dataloader <br>\n",
    "    del test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
